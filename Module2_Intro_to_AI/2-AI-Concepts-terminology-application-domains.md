# AI concepts and applications

## What is Cognitive AI? ðŸ§ 

**Cognitive AI** is a broad branch of artificial intelligence designed to **mimic human thought processes** involved in decision-making and problem-solving, going beyond simple pattern recognition.

### Key Goals and Characteristics:

* **Mimics Cognition:** Its goal is to simulate human functions like **learning, reasoning, understanding context, and making judgments** based on evidence.
* **Focus on Insight:** It excels at analyzing large volumes of **unstructured data** (text, images, speech, etc.) to **extract context, draw logical conclusions (reasoning),** and **provide recommendations.**
* **Augmentation (Decision Support):** Cognitive systems are often designed to *augment* human intelligenceâ€”that is, to provide insights and potential solutions to *help* a human expert make a more informed, complex decision, rather than simply making the decision autonomously.
* **Adaptability:** It's designed to be **adaptive** and can dynamically adjust to new information, ambiguities, and unexpected scenarios without needing a full re-write of its rules.

**Example:** An IBM Watson-like system analyzing a patient's medical history, lab results, and millions of research papers to suggest potential diagnoses and treatment plans to a doctor. It's reasoning through complex evidence.

***

## 1. Google Translate

| AI Type Focus | The Role in Google Translate | Cognitive/Generative? |
| :--- | :--- | :--- |
| **Generative Focus (LLMs)** | Modern translation is essentially **generating** a new sentence in the target language based on the statistical patterns (the structure and flow) learned from billions of paired sentences. | **Strongly Generative** (using NMTâ€”Neural Machine Translation) |
| **Cognitive Focus** | While the *translation output* is generative, the system must **reason** to handle ambiguity, such as selecting the right meaning for a word with multiple possible translations (e.g., "staff" as in personnel vs. "staff" as in a stick). Modern systems do this by considering the **entire context** of the sentence and sometimes the surrounding text, which mimics a cognitive process of **semantic reasoning** and **contextual understanding**. | **Involves Cognitive Elements** |

**The key distinction:** When Google Translate produces a translation that is *fluent* and *contextually correct*, it's because it used a neural network (Generative AI) that was trained to **mimic the cognitive process** of understanding context and semantic relationships. It doesn't just swap words; it builds a mental model of the source sentence's *meaning* before generating the target sentence.


## 2. Amazon Alexa (and other virtual assistants)

| AI Type Focus | The Role in Amazon Alexa | Cognitive/Generative? |
| :--- | :--- | :--- |
| **Traditional/Narrow AI** | The "Classic Alexa" only recognizes specific commands ("Alexa, turn on the lights"). This is a narrow, rule-based function. | **Narrow/Traditional AI** |
| **Cognitive Focus** | The system must **reason** about your **intent**. If you say, "Play that song I was listening to yesterday," it must use context (your history) and logic (determining "yesterday's song") to decide the action. Newer versions (like "Alexa+") are designed to handle complex, multi-step requests and **reason** through a series of steps to fulfill a goal. | **Heavily Cognitive** |
| **Generative Focus (Newer Versions)** | When the assistant has a **natural, free-flowing conversation** or *generates* a personalized story or a lengthy, detailed answer to a question, it is powered by an LLM (Generative AI). | **Increasingly Generative** |

**The key distinction:**

* **Cognitive AI** is the part of Alexa that **learns your habits** (context), understands your **intent** (reasoning), and connects multiple systems (like the smart thermostat and the lights) to solve a **complex, real-world problem**. Your observation about understanding **tone** (emotion/sentiment analysis) would also fall under the **Perception and Reasoning** aspects of Cognitive AI.
* **Generative AI** is what makes its *speech* sound natural and its *responses* go beyond canned scripts.

**In summary:** When a system like Google Translate or Amazon Alexa goes beyond a simple look-up and seems to *understand* what you mean in a complex, ambiguous situation, it is engaging in **Cognitive AI**, even if the final output (the translated sentence or the conversational response) is generated by a **Generative AI** model. The two often work together.

# Terminologies and concepts related to artificial intelligence (AI).

**Machine Learning and Deep Learning**
- Machine learning is a subset of AI that uses algorithms to analyze data and make decisions without explicit programming, enabling autonomous problem-solving.
- Deep learning, a specialized subset of machine learning, employs multi-layered neural networks to analyze complex data and simulate human decision-making.

**Neural Networks**
- Neural networks are computational models inspired by the human brain, consisting of interconnected nodes organized in three layers: input, hidden, and output.
- These networks process raw data, perform complex computations, and produce results, contributing to the overall functionality of AI systems.

# Maching learning
* instead of being given rules or an algorithm, ml creates its own - it learns from data

- **What is Machine Learning?**
  - A subset of Artificial Intelligence (AI).
  - Uses computer algorithms to analyze data and make decisions.

- **How Does It Work?**
  - Instead of following fixed rules, it builds models from data.
  - Models learn from data to classify and predict outcomes.

- **Types of Machine Learning:**
  - **Supervised Learning:**
    - Trains on labeled data (data with known answers).
    - Example: Teaching a model to recognize pictures of cats and birds using labeled images.
  
  - **Unsupervised Learning:**
    - Works with unlabeled data (data without known answers).
    - Example: Grouping similar items together, like clustering customers based on buying habits.

  - **Reinforcement Learning:**
    - Learns by trying different actions and receiving rewards or penalties.
    - Example: Teaching a machine to play chess by rewarding it for winning moves.

# Machine Learning techniques and training
* Supervised learning can be broken down into regression, classification, and nns
* regression is discovering relationships between features x and y eg the greater the age, the greater the salary (used for continuous values)
* classification is based on discrete values eg based on discrete values like age,bmi detect if they are healthy (true or false). Classification allows us to extract features from the data and determine a result. Predict class of input.
* Data sets are usually devided into: training sets (train algorithm), validation sets (helps validate and adjust params), and test sets (asses performance)
- **Deep Learning (DL)** is a specialized **subset of Machine Learning (ML)** that uses **Artificial Neural Networks (ANNs)** with multiple layers (hence "deep") to analyze data.
- It models the way the human brain processes information by using layers of interconnected **nodes (neurons)**.
- Each layer in a **Neural Network (NN)** processes the data passed to it from the previous layer, extracting increasingly complex features.
- DL excels at learning directly from **unstructured data** (e.g., images, audio, video, raw text), eliminating the need for extensive manual feature engineering.
- Key capabilities include **Natural Language Understanding (NLU)**, image recognition, voice transcription, medical imaging, and language translation.
- Building a DL model involves:
    - Designing the NN architecture (number of layers, nodes per layer).
    - **Configuring each layer** and selecting **activation functions** to determine the output of the nodes.
    - Providing the model with a large set of **training examples** (data).
- The network learns by adjusting the **weights** (strength of connections) and **biases** of the nodes.
    - The goal is to **minimize the loss function** and improve prediction **accuracy** via an optimization algorithm like **backpropagation**.
- A significant advantage of DL is its **scalability with data volume**.
    - Unlike many traditional ML algorithms, which often plateau or degrade in performance after a certain point, DL models generally **continue to improve** as they are trained on **more data**.
- DL is a core technology for complex applications like **autonomous vehicles (driverless cars)**. 

# Neural Networks
* consists of neurons whivh take in data and learn by making decisions and adjusting how they learn over time
* consists of an input layer, an ouput layer, and one or more hidden layers. 
* Input layer takes in the data, hidden layer processes the data by applying an activation function, output layer produces the final result
* Activation functions are mathematical functions that allow the network to learn complex patterns
* Training:
    - forward progpagation is when the data passes from input to output layer through the hidden layers in a forward motion (no feedback) and an output is produced
    - the output is then compared to the expected output to calculate the error/ loss (difference between output and the one we expect)
    - backpropgation is then applied which passes the error back through the network to update bias/ weights with an aim to reduce future errors
    - Repeat until accurate
* various types of NNs:
    - perceptron NNs: simplest type consisting of only input and output
    - feed-forward NNs: info flows in only one direction. 
    - Deep feed-forward NNs: many hidden layers
    - Modular NNs: combines 2+ NNs to product output
    - Convoluional NNs: good for analysis visual data. Convolution refers to a mathematical function where a function is applied to another function and the result is a combination of the two functions. In CNNs, this process takes place through multiple layers, with each layer performing a convolutional on the output from the previous layer
    - Recurrent NNs: each input is received with a specific delay in time which gives the layer time to consider the context ???? can use this when you need to access previous info in a current iteration eg its useful for predicting the next word in a sentence (chat model) as it considers the context of the conversation before producing results

# Machine learning v deep learning
* ml is sub field of ai then nn is subfield of ml then nns make up dl
* weights determine how much an input contributes to the output. if a value has a greaster weight, it will contribute more to the output
* NN is deep if it consists of more than 3 layers which includes the input and output layer
* classical ml requires more human interaction to learn eg we tell the model what distinguises a cat from a dog (supervised). Deep ml doesnt required labels and it can automatically determine distinguishing features

# Generative ai models
* different types depending on what theyre meant to do. 
    - types:
        - Variational autoencoders (VAEs): transform input data through encoding and decoding. They have 3 main parts; encoder network, latent space and a decoder network. encoder takes the input data and transforms it to a simpler form called the latent space representation which holds the key features of the data. The decoder then uses this latent space data to create new outputs. Applications include image generation, anomoly detection eg the Fashion MNIST VAE is used to create new fashion items based on a dtaa set
        - Generative adversarial networks (GANs): Gans involve two NNs, the generator and the discriminator. Generator creates new data samples and the discriminator tries to find out if its real or fake. This process continues until the generator can produce data thats so good that the discriminator can no longer tell the difference between the real and fake data. Can be used for image synthesis, data augmentation, style transfer???? wtf are these big example is NVDIAs styleGan which can produce relaistic images of faces, styles etc.
        - Autoregressive models: create data sequentially considering the context of previously generated data. They predict the next element in a sequence. can generate text, music eg wavenet
        - Transformers - used in nlp tasks. consist of encoder and decoder layers which generate text sequences or cross-language translations. GPT and google gemini are transformers that can generate text
* Gen ai can usually be categorised into unimodal and multimodal models
* unimodal models generate output in the same form as the input but multimodal produces it in a diff  eg gpt3 is unimodal but dale is multimodal

# NLP, speech, and computer vision
**Natural Language Processing (NLP)**
- NLP helps computers understand and produce human language.
- It uses machine learning to analyze the meaning of words in context.

**Speech Technology**
- Speech-to-text (STT) converts spoken words into written text, enabling voice commands and transcription.
- Text-to-speech (TTS) turns written text into spoken words, allowing for natural interaction with devices.

**Computer Vision**
- Computer vision enables machines to interpret visual data, like images and videos.
- It is used in various industries for tasks like facial recognition, object detection, and image classification.

# NLP
* Strucutred data might be something like "Add eggs to my shopping list". Although this might make sense to us, its still unstructured and it might not make sense to a ml model. strutured might be something like you have a shopping list object which might have ttributes like items and we might want to add some items to it.
* going from unstructured to structured is called natural language understanding (NLU) because it converts our language to a language the model understands
* Going from structured to unstructured is called Natural Language Generation (NLG) because the model is producing natual language from stuff it understands
* Applications: 
    * Machine translation: things might not always directly translate between languages ie you dont necessarily go from word to word to translate eg a phrase in english needs to be translated to its equivalent in another language otherwise it might not make sense
    * Virtual agents: to chat in natural language
    * Sentiment analysis: extract meaning from words
    * spam detection
* Process:
    * Tokenisation: taking text and breaking it down into chunks
    * stemming
    * lemmatisation: kinda like stemming but more accurate eg university might stem to college instead of universe
    * Part of speech tagging: looking at where the token is used in the context of the sentence
    * Named Entity Recognition (NER): is there an entity assosiated with the token eg nicole might have an entity of a person

# Self driving cars
* One of the main objective is object recognition (computer vision)
* They take in lots of types of data like vision, radar, laser data?? and fuse that together to detect objects